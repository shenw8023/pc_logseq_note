- http://jalammar.github.io/illustrated-gpt2/
- GPT-2 does not re-interpret the first token in light of the second token.
- This is what self-attention does. It bakes in the modelâ€™s understanding of relevant and associated words that explain the context of a certain word before processing that word (passing it through a neural network). It does that by assigning scores to how relevant each word in the segment is, and adding up their vector representation.