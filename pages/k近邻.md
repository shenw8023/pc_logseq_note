- ## 基本概念
- k-NN是一种基本分类与回归方法，这里讨论分类问题
- 输入：实例的特征向量，对应于特征空间的点
- 输出：实例的类别，可以取多类
- 基本思路：假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。
- 不具有显式的学习过程，**实际上利用训练数据集对特征向量空间进行划分，并作为其分类的模型**
- 三个基本要素：
	- k值的选择
	- 距离度量
	- 分类决策规则
-
- ## 距离度量
- 特征空间中两个实例点的距离是两个实例点的相似程度的反映
- 特征空间一般为n维的实数向量空间，使用欧式距离或其他如 L_p距离 （公式3.2）
-
- ## k值的选择
- 较小的k值，相当于用较小的领域中的训练实例进行预测，学习的近似误差会减小，估计误差会增大，意味着整体模型变得复杂，容易过拟合。
- 较大的k值，相当于用较大的领域中的训练实例进行预测，学习的估计误差会减小，但是近似误差会增大，意味着整体模型变得简单。
- 应用中，一般取一个较小的值，采用交叉验证法来选取最优的k值
-
- ## 分类决策规则
- 往往是多数表决
- 多数表决等价于经验风险最小化
-
- ## k近邻法的实现：kd树
- ### 构造kd树
- 解决如何对训练数据进行快速k近邻搜索的问题，肯定不能对每个目标点都进行线性搜索
- 是一种存储k维空间数据的树结构
- kd树是二叉树，**表示对k维空间的一种划分**
- 构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列k维超矩形区域，k**d树的每个节点对应一个k维超矩形区域。**
- 构造方式：
	- 不断地选择坐标轴，和在此坐标轴上的一个切分点，垂直坐标轴切分，确定一个超平面
	- 通常，依次选择坐标轴进行切分
	- 选择训练实例点在选定坐标轴上的中位数为切分点
	- 直到子区域内没有实例时终止
-
- ### 搜索kd树
- 如何利用kd树进行k近邻搜索
- 过程：
	- 选定一个目标点，搜索其最近邻，首先找到包含目标点的叶节点，然后从该叶节点出发，**依次回退到父节点**，不断查找与目标点最邻近的节点，当确定不可能存在更近的节点时终止
- ## 总结：
- k近邻法中，当训练集，距离度量，k值及分类决策规则确定后，其结果唯一确定
- kd树是一种便于对k维空间中的数据进行快速检索的数据结构